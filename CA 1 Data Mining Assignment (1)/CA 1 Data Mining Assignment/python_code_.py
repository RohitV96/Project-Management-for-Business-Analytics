# -*- coding: utf-8 -*-
"""Python code .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HZRLkPTM13usYSDE-n5Jt9TOYIXgTs_W
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from imblearn.over_sampling import SMOTE  # imblearn library can be installed using pip install imblearn
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neural_network import MLPClassifier
from imblearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
from sklearn.linear_model import SGDClassifier
import warnings
warnings.filterwarnings("ignore", category=FutureWarning)

df = pd.read_csv("/content/drive/MyDrive/online_shoppers_intention.csv")
df

df.info()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

df['Month'] = le.fit_transform(df['Month'])
df['VisitorType'] = le.fit_transform(df['VisitorType'])
df['Weekend'] = le.fit_transform(df['Weekend'])

df.info()

from sklearn.model_selection import train_test_split

# Split the data into features (X) and target variable (y)
X = df.drop('Revenue', axis=1)
y = df['Revenue']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import LogisticRegression


# Create a Logistic Regression classifier
logreg_classifier = LogisticRegression()

# Train the classifier
logreg_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_logreg = logreg_classifier.predict(X_test)

# Evaluate the classifier
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test,y_pred_logreg))
print(classification_report(y_test,y_pred_logreg))

# Implementing Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier()

# Train the classifier
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_classifier.predict(X_test)

# Evaluate the classifier
accuracy_rf = accuracy_score(y_test, y_pred_rf)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred_rf))
print(classification_report(y_test, y_pred_rf))

from sklearn.ensemble import AdaBoostClassifier

# Create an AdaBoost classifier
adaboost_classifier = AdaBoostClassifier()

# Train the classifier
adaboost_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_adaboost = adaboost_classifier.predict(X_test)

# Evaluate the classifier
accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred_adaboost))
print(classification_report(y_test, y_pred_adaboost))

from sklearn.svm import SVC

# Create an SVC classifier
svc_classifier = SVC()

# Train the classifier
svc_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_svc = svc_classifier.predict(X_test)

# Evaluate the classifier
accuracy_svc = accuracy_score(y_test, y_pred_svc)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred_svc))
print(classification_report(y_test, y_pred_svc))

from sklearn.neural_network import MLPClassifier

# Create an MLP classifier
mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,))

# Train the classifier
mlp_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred_mlp = mlp_classifier.predict(X_test)

# Evaluate the classifier
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred_mlp))
print(classification_report(y_test, y_pred_mlp))

# Compare accuracies of all classifiers
accuracies = [accuracy_rf, accuracy_adaboost, accuracy_svc, accuracy_logreg, accuracy_mlp]
best_model_index = accuracies.index(max(accuracies))
best_model_name = ['Random Forest', 'AdaBoost', 'SVC', 'Logistic Regression', 'MLP'][best_model_index]

print("Best Model: ", best_model_name)

# Cost assumptions
average_revenue_per_sale = 100
average_cost_per_false_negative = average_revenue_per_sale

# Calculate the expected cost if no model is deployed
num_false_negatives = sum((y_test == 1) & (y_pred_rf != 1))
expected_cost_no_model = num_false_negatives * average_cost_per_false_negative

print("Expected Cost (No Model Deployed): $", expected_cost_no_model)

# Cost assumptions
average_cost_per_false_positive = 20

# Calculate the expected savings if the best model is deployed
num_false_positives = sum((y_test != 1) & (y_pred_rf == 1))
expected_savings_best_model = num_false_positives * average_cost_per_false_positive

print("Expected Savings (Best Model Deployed): $", expected_savings_best_model)