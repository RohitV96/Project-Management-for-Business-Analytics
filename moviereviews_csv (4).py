# -*- coding: utf-8 -*-
"""MovieReviews.csv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JTun3xRqVTh1TKQEIYlugGAKUpfMBxRU
"""

# Conversion to structured format/ TF-IDF Matrix – Discuss how tuning
# hyperparameters ‘ngram_range’ and ‘min_df’ affected the TF-IDF matrix and
# classifiers’ performance.

#importing the libraries
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from wordcloud import WordCloud,STOPWORDS
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize,sent_tokenize
from bs4 import BeautifulSoup
import spacy
import re,string,unicodedata
from nltk.tokenize.toktok import ToktokTokenizer
from nltk.stem import LancasterStemmer,WordNetLemmatizer
from sklearn.linear_model import LogisticRegression,SGDClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from textblob import TextBlob
from textblob import Word
import re, nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('omw-1.4')
nltk.download('wordnet')
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn import metrics
from sklearn.svm import LinearSVC
import joblib

# Reading dataset as dataframe
df = pd.read_csv("/content/drive/MyDrive/MovieReviews.csv")
pd.set_option('display.max_colwidth', None) # Setting this so we can see the full content of cells
pd.set_option('display.max_columns', None) # to make sure we can see all the columns in output window

df.shape

# Cleaning review
def cleaner(review):
    soup = BeautifulSoup(review, 'lxml') # removing HTML entities such as ‘&amp’,’&quot’,'&gt'; lxml is the html parser and shoulp be installed using 'pip install lxml'
    souped = soup.get_text()
    re1 = re.sub(r"(@|http://|https://|www|\\x)\S*", " ", souped) # substituting @mentions, urls, etc with whitespace
    re2 = re.sub("[^A-Za-z]+"," ", re1) # substituting any non-alphabetic character that repeats one or more times with whitespace

    """
    For more info on regular expressions visit -
    https://docs.python.org/3/howto/regex.html
    """

    tokens = nltk.word_tokenize(re2)
    lower_case = [t.lower() for t in tokens]

    stop_words = set(stopwords.words('english'))
    filtered_result = list(filter(lambda l: l not in stop_words, lower_case))

    wordnet_lemmatizer = WordNetLemmatizer()
    lemmas = [wordnet_lemmatizer.lemmatize(t,'v') for t in filtered_result]
    return lemmas

df['cleaned_MovieReviews'] = df.review.apply(cleaner)
df = df[df['cleaned_MovieReviews'].map(len) > 0] # removing rows with cleaned tweets of length 0

df

df.shape

# Saving cleaned reviews to csv
df.to_csv('cleaned_data.csv', index=False)
df['cleaned_MovieReviews'] = [" ".join(row) for row in df['cleaned_MovieReviews'].values] # joining tokens to create strings. TfidfVectorizer does not accept tokens as input
data = df['cleaned_MovieReviews']
Y = df['sentiment'] # target column
tfidf = TfidfVectorizer(min_df=.0006, ngram_range=(1,3)) # min_df=.0006 means that each ngram (unigram, bigram, & trigram) must be present in at least 30 documents for it to be considered as a token (200000*.00015=30). This is a clever way of feature engineering
tfidf.fit(data) # learn vocabulary of entire data
data_tfidf = tfidf.transform(data) # creating tfidf values
pd.DataFrame(pd.Series(tfidf.get_feature_names_out())).to_csv('vocabulary.csv', header=False, index=False)
print("Shape of tfidf matrix: ", data_tfidf.shape)

from collections import Counter
Counter(Y)

import time
start_svc = time.time()
# Implementing Support Vector Classifier
model = LinearSVC() # kernel = 'linear' and C = 1# Running cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0
for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)
    X_train, Y_train = data_tfidf[train_index], Y[train_index]
    X_test, Y_test = data_tfidf[test_index], Y[test_index]
    model.fit(X_train, Y_train) # Fitting SVC
    Y_pred = model.predict(X_test)
    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy
    print("Cross-validation accuracy: ", score)
    scores.append(score) # appending cross-validation accuracy for each iteration
end_svc = time.time()
time_taken = end_svc - start_svc
print('time taken is' ,time_taken )
svc_mean_accuracy = np.mean(scores) # min_df=.0015 accuracy is 0.88, min_df=.0006 accuracy is 0.89, min_df=.0086 accuracy is 0.87
print("Mean cross-validation accuracy: ", svc_mean_accuracy)

# Implementing Naive Bayes Classifier
nbc_clf = MultinomialNB()
import time
start_nbc = time.time()
# Running cross-validation
kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation
scores=[]
iteration = 0
for train_index, test_index in kf.split(data_tfidf, Y):
    iteration += 1
    print("Iteration ", iteration)
    X_train, Y_train = data_tfidf[train_index], Y.iloc[train_index]
    X_test, Y_test = data_tfidf[test_index], Y.iloc[test_index]
    nbc_clf.fit(X_train, Y_train) # Fitting NBC
    Y_pred = nbc_clf.predict(X_test)
    score = metrics.accuracy_score(Y_test, Y_pred) # Calculating accuracy
    print("Cross-validation accuracy: ", score) # min_df=.0015 accuracy is 0.86, min_df=.0006 accuracy is 0.87, min_df=.0086 accuracy is 0.85
    scores.append(score) # appending cross-validation accuracy for each iteration
nbc_mean_accuracy = np.mean(scores)
print("Mean cross-validation accuracy: ", nbc_mean_accuracy)
end_nbc = time.time()
time_taken = end_nbc - start_nbc
print('time taken is' ,time_taken )

if svc_mean_accuracy > nbc_mean_accuracy:
  clf = LinearSVC().fit(data_tfidf, Y)
  joblib.dump(clf, 'svc.sav')
else:
  clf = MultinomialNB().fit(data_tfidf, Y)
  joblib.dump(clf, 'nbc.sav')

df.describe()

# Task 2
# Using the given dataset, create two Word-Clouds – one containing 10 MOST USED WORDS in
# the positive sentiment reviews and the other containing 10 MOST USED WORDS in the
# negative sentiment reviews.

# Create a DataFrame
df = pd.DataFrame(data)

# Filter positive and negative reviews
positive_text = " ".join(df[df['sentiment'] == 'positive']['cleaned_MovieReviews'])
negative_text = " ".join(df[df['sentiment'] == 'negative']['cleaned_MovieReviews'])

# Generate Word Cloud for 10 most used words in positive sentiment
positive_wordcloud = WordCloud(max_words=10, width=400, height=300, background_color='white').generate(positive_text)

# Generate Word Cloud for 10 most used words in negative sentiment
negative_wordcloud = WordCloud(max_words=10, width=400, height=300, background_color='white').generate(negative_text)

# Plotting
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(positive_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Positive Review Word Cloud")

plt.subplot(1, 2, 2)
plt.imshow(negative_wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title("Negative Review Word Cloud")

plt.tight_layout()
plt.show()

